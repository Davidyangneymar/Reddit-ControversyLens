"""
Redditæ•°æ®åŠ è½½ä¸é¢„å¤„ç†æ¨¡å—

æ”¯æŒåŠ è½½Kaggle Redditè¯„è®ºæ•°æ®é›†:
https://www.kaggle.com/datasets/smagnan/1-million-reddit-comments-from-40-subreddits
"""

import os
import re
import pandas as pd
import numpy as np
from typing import List, Dict, Optional, Tuple, Generator
from collections import Counter
from datetime import datetime

from .config import Config


class TextPreprocessor:
    """æ–‡æœ¬é¢„å¤„ç†å™¨"""
    
    def __init__(self, config: Dict = None):
        self.config = config or Config.PREPROCESSING
        self.stopwords = Config.get_all_stopwords()
        
        # ç¼–è¯‘æ­£åˆ™è¡¨è¾¾å¼
        self.url_pattern = re.compile(
            r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+'
        )
        self.mention_pattern = re.compile(r'@\w+')
        self.special_pattern = re.compile(r'[^\w\s]')
        self.whitespace_pattern = re.compile(r'\s+')
        self.reddit_quote_pattern = re.compile(r'^>.*$', re.MULTILINE)
        # Reddit Markdown è¡¨æ ¼å’Œæ ¼å¼ç¬¦å·
        self.markdown_table_pattern = re.compile(r'\|.*\|')  # è¡¨æ ¼è¡Œ
        self.markdown_separator_pattern = re.compile(r':?-{2,}:?')  # è¡¨æ ¼åˆ†éš”ç¬¦ :---, ---, ---:
        self.markdown_format_pattern = re.compile(r'\*{1,2}[^*]+\*{1,2}')  # **bold** *italic*
        self.repeated_chars_pattern = re.compile(r'(.)\1{3,}')  # é‡å¤å­—ç¬¦ aaaa, ----
        
    def clean_text(self, text: str) -> str:
        """æ¸…ç†æ–‡æœ¬"""
        if not isinstance(text, str):
            return ""
        
        # ç§»é™¤Redditå¼•ç”¨
        text = self.reddit_quote_pattern.sub('', text)
        
        # ç§»é™¤Markdownè¡¨æ ¼
        text = self.markdown_table_pattern.sub(' ', text)
        text = self.markdown_separator_pattern.sub(' ', text)
        
        # ç§»é™¤Markdownæ ¼å¼ç¬¦å·
        text = self.markdown_format_pattern.sub(' ', text)
        
        # ç§»é™¤é‡å¤å­—ç¬¦æ¨¡å¼
        text = self.repeated_chars_pattern.sub(' ', text)
        
        # ç§»é™¤ HTML å®ä½“ (&amp; &gt; &lt; &#x200b; ç­‰)
        text = re.sub(r'&[a-zA-Z]+;', ' ', text)
        text = re.sub(r'&#x?[0-9a-fA-F]+;?', ' ', text)
        text = re.sub(r'x[0-9a-fA-F]{4}', ' ', text)  # x200b ç­‰
        
        # ç§»é™¤URL
        if self.config.get("remove_urls", True):
            text = self.url_pattern.sub(' ', text)
        
        # ç§»é™¤@mentions
        if self.config.get("remove_mentions", True):
            text = self.mention_pattern.sub(' ', text)
        
        # è½¬å°å†™
        if self.config.get("lowercase", True):
            text = text.lower()
        
        # ç§»é™¤ç‰¹æ®Šå­—ç¬¦(ä¿ç•™åŸºæœ¬æ ‡ç‚¹)
        text = re.sub(r'[^a-zA-Z0-9\s\'\-]', ' ', text)
        
        # è§„èŒƒåŒ–ç©ºç™½
        text = self.whitespace_pattern.sub(' ', text).strip()
        
        return text
    
    def tokenize(self, text: str) -> List[str]:
        """åˆ†è¯"""
        text = self.clean_text(text)
        tokens = text.split()
        
        # ç§»é™¤åœç”¨è¯å’Œæ— æ„ä¹‰token
        if self.config.get("remove_stopwords", True):
            filtered_tokens = []
            for t in tokens:
                # è·³è¿‡åœç”¨è¯
                if t in self.stopwords:
                    continue
                # è·³è¿‡å¤ªçŸ­çš„token
                if len(t) <= 1:
                    continue
                # è·³è¿‡çº¯æ•°å­—
                if t.isdigit():
                    continue
                # è·³è¿‡åªåŒ…å«ç‰¹æ®Šå­—ç¬¦çš„token (--- , *** , ===)
                if re.match(r'^[\-\*\=\_\|\:]+$', t):
                    continue
                # è·³è¿‡åŒ…å«è¿‡å¤šé‡å¤å­—ç¬¦çš„token
                if re.search(r'(.)\1{2,}', t):
                    continue
                filtered_tokens.append(t)
            tokens = filtered_tokens
        
        return tokens
    
    def get_ngrams(self, tokens: List[str], n: int) -> List[str]:
        """æå–n-gram"""
        if len(tokens) < n:
            return []
        return [' '.join(tokens[i:i+n]) for i in range(len(tokens) - n + 1)]
    
    def get_all_ngrams(self, tokens: List[str], ngram_range: Tuple[int, int] = (1, 3)) -> List[str]:
        """æå–æŒ‡å®šèŒƒå›´çš„æ‰€æœ‰n-gram"""
        all_ngrams = []
        for n in range(ngram_range[0], ngram_range[1] + 1):
            all_ngrams.extend(self.get_ngrams(tokens, n))
        return all_ngrams
    
    def get_shingles(self, text: str, k: int = 5) -> set:
        """è·å–å­—ç¬¦çº§k-shingleé›†åˆ(ç”¨äºMinHash)"""
        text = self.clean_text(text)
        if len(text) < k:
            return {text} if text else set()
        return {text[i:i+k] for i in range(len(text) - k + 1)}


class RedditDataLoader:
    """Redditæ•°æ®åŠ è½½å™¨"""
    
    def __init__(self, data_path: str = None):
        """
        åˆå§‹åŒ–æ•°æ®åŠ è½½å™¨
        
        Args:
            data_path: CSVæ–‡ä»¶è·¯å¾„ã€‚å¦‚æœä¸æä¾›ï¼Œå°†åœ¨Config.DATA_DIRä¸­æŸ¥æ‰¾
        """
        self.data_path = data_path
        self.preprocessor = TextPreprocessor()
        self.df = None
        self.subreddits = []
        
    def find_data_file(self) -> str:
        """æŸ¥æ‰¾æ•°æ®æ–‡ä»¶"""
        if self.data_path and os.path.exists(self.data_path):
            return self.data_path
        
        # åœ¨dataç›®å½•ä¸­æŸ¥æ‰¾
        data_dir = Config.DATA_DIR
        if os.path.exists(data_dir):
            # ä¼˜å…ˆæŸ¥æ‰¾é…ç½®çš„æ–‡ä»¶
            config_file = os.path.join(data_dir, Config.REDDIT_DATA_FILE)
            if os.path.exists(config_file):
                return config_file
            
            # æŸ¥æ‰¾å…¶ä»–CSVæ–‡ä»¶
            for f in os.listdir(data_dir):
                if f.endswith('.csv') and ('reddit' in f.lower() or 'kaggle' in f.lower() or 'RC' in f):
                    return os.path.join(data_dir, f)
        
        return None
    
    def load(self, 
             num_comments: int = None,
             subreddits: List[str] = None,
             min_score: int = None,
             random_sample: bool = True) -> pd.DataFrame:
        """
        åŠ è½½Redditè¯„è®ºæ•°æ® (ä¼˜åŒ–ç‰ˆï¼šå…¨é‡æ•°æ®å¿«é€ŸåŠ è½½)
        
        Args:
            num_comments: åŠ è½½çš„è¯„è®ºæ•°é‡(Noneè¡¨ç¤ºå…¨éƒ¨)
            subreddits: æŒ‡å®šçš„subredditåˆ—è¡¨
            min_score: æœ€å°è¯„åˆ†è¿‡æ»¤
            random_sample: æ˜¯å¦éšæœºé‡‡æ ·
            
        Returns:
            å¤„ç†åçš„DataFrame
        """
        print("\nğŸ“‚ åŠ è½½Redditæ•°æ®...")
        
        # æŸ¥æ‰¾æ•°æ®æ–‡ä»¶
        data_file = self.find_data_file()
        if not data_file:
            print("âš ï¸ æœªæ‰¾åˆ°Redditæ•°æ®æ–‡ä»¶ï¼Œå°†ç”Ÿæˆæ¨¡æ‹Ÿæ•°æ®")
            return self._generate_sample_data(num_comments or 10000)
        
        print(f"   æ–‡ä»¶: {data_file}")
        
        # åŠ è½½CSV
        try:
            # å…ˆè¯»å–ä¸€å°éƒ¨åˆ†ç¡®å®šåˆ—å
            sample_df = pd.read_csv(data_file, nrows=5)
            print(f"   åˆ—å: {list(sample_df.columns)}")
            
            # ç¡®å®šè¦è¯»å–çš„è¡Œæ•°
            if num_comments:
                if random_sample and num_comments < 500000:
                    # å°è§„æ¨¡é‡‡æ ·æ—¶éšæœº
                    total_rows = sum(1 for _ in open(data_file, 'r', encoding='utf-8', errors='ignore')) - 1
                    print(f"   æ€»è¡Œæ•°: {total_rows:,}")
                    
                    skip_rows = sorted(np.random.choice(
                        range(1, total_rows + 1), 
                        size=max(0, total_rows - num_comments), 
                        replace=False
                    ))
                    self.df = pd.read_csv(data_file, skiprows=skip_rows)
                else:
                    # å¤§è§„æ¨¡ç›´æ¥è¯»å–å‰Nè¡Œ
                    self.df = pd.read_csv(data_file, nrows=num_comments)
            else:
                # å…¨é‡æ•°æ®ï¼šä½¿ç”¨ä¼˜åŒ–å‚æ•°å¿«é€ŸåŠ è½½
                print("   å…¨é‡åŠ è½½ä¸­...")
                self.df = pd.read_csv(
                    data_file,
                    dtype={'subreddit': 'category', 'controversiality': 'int8'},  # ä¼˜åŒ–å†…å­˜
                    low_memory=False
                )
            
            print(f"   åŠ è½½: {len(self.df):,} æ¡è¯„è®º")
            
        except Exception as e:
            print(f"âš ï¸ åŠ è½½å¤±è´¥: {e}")
            print("   å°†ç”Ÿæˆæ¨¡æ‹Ÿæ•°æ®")
            return self._generate_sample_data(num_comments or 10000)
        
        # æ ‡å‡†åŒ–åˆ—å
        self.df = self._standardize_columns(self.df)
        
        # è¿‡æ»¤
        if subreddits:
            self.df = self.df[self.df['subreddit'].isin(subreddits)]
            print(f"   Subredditè¿‡æ»¤å: {len(self.df):,}")
        
        if min_score is not None:
            self.df = self.df[self.df['score'] >= min_score]
            print(f"   è¯„åˆ†è¿‡æ»¤å: {len(self.df):,}")
        
        # é¢„å¤„ç† (å¤§æ•°æ®é‡æ—¶ä½¿ç”¨å¹¶è¡Œå¤„ç†)
        if len(self.df) > 50000:
            self.df = self._preprocess_parallel(self.df)
        else:
            self.df = self._preprocess(self.df)
        
        # ç»Ÿè®¡ä¿¡æ¯
        self.subreddits = self.df['subreddit'].unique().tolist()
        print(f"   Subreddits: {len(self.subreddits)}")
        print(f"   æœ‰æ•ˆè¯„è®º: {len(self.df):,}")
        
        return self.df
    
    def _standardize_columns(self, df: pd.DataFrame) -> pd.DataFrame:
        """æ ‡å‡†åŒ–åˆ—å"""
        # å¸¸è§çš„åˆ—åæ˜ å°„
        column_mapping = {
            'body': 'text',
            'comment': 'text',
            'content': 'text',
            'comment_body': 'text',
            'selftext': 'text',
            'ups': 'score',
            'upvotes': 'score',
            'points': 'score',
            'created': 'timestamp',
            'created_utc': 'timestamp',
            'date': 'timestamp',
            'author': 'author',
            'user': 'author',
            'username': 'author',
            'sub': 'subreddit',
            'sr': 'subreddit',
        }
        
        # è½¬å°å†™å¹¶æ˜ å°„
        df.columns = df.columns.str.lower()
        df = df.rename(columns={k: v for k, v in column_mapping.items() if k in df.columns})
        
        # ç¡®ä¿å¿…è¦åˆ—å­˜åœ¨
        if 'text' not in df.columns:
            # å°è¯•æ‰¾åˆ°æ–‡æœ¬åˆ—
            text_cols = [c for c in df.columns if 'text' in c or 'body' in c or 'comment' in c]
            if text_cols:
                df['text'] = df[text_cols[0]]
            else:
                df['text'] = df.iloc[:, 0].astype(str)
        
        if 'subreddit' not in df.columns:
            df['subreddit'] = 'unknown'
        
        if 'score' not in df.columns:
            df['score'] = 0
        
        if 'id' not in df.columns:
            df['id'] = range(len(df))
        
        if 'timestamp' not in df.columns:
            df['timestamp'] = pd.Timestamp.now()
        
        return df
    
    def _preprocess(self, df: pd.DataFrame) -> pd.DataFrame:
        """é¢„å¤„ç†æ•°æ® (ä¼˜åŒ–ç‰ˆæœ¬ï¼šæ‰¹é‡å¤„ç†)"""
        print("   é¢„å¤„ç†ä¸­...")
        
        # ç§»é™¤ç©ºè¯„è®º
        df = df.dropna(subset=['text'])
        df = df[df['text'].str.len() > 0]
        
        # é•¿åº¦è¿‡æ»¤
        min_len = Config.PREPROCESSING.get('min_comment_length', 10)
        max_len = Config.PREPROCESSING.get('max_comment_length', 5000)
        df = df[df['text'].str.len().between(min_len, max_len)]
        
        # ç§»é™¤[deleted]å’Œ[removed]
        df = df[~df['text'].str.contains(r'^\[deleted\]$|^\[removed\]$', regex=True, na=False)]
        
        # æ‰¹é‡é¢„å¤„ç† - ä½¿ç”¨å‘é‡åŒ–æ“ä½œ
        print("   æ‰¹é‡æ¸…ç†æ–‡æœ¬...")
        df['clean_text'] = df['text'].apply(self.preprocessor.clean_text)
        
        print("   æ‰¹é‡åˆ†è¯...")
        df['tokens'] = df['clean_text'].apply(lambda x: self.preprocessor.tokenize(x))
        df['token_count'] = df['tokens'].apply(len)
        
        # ç§»é™¤tokenè¿‡å°‘çš„è¯„è®º
        df = df[df['token_count'] >= 3]
        
        # é‡ç½®ç´¢å¼•
        df = df.reset_index(drop=True)
        
        return df
    
    def _preprocess_parallel(self, df: pd.DataFrame, n_jobs: int = -1) -> pd.DataFrame:
        """
        å¹¶è¡Œé¢„å¤„ç†æ•°æ® (å¤§æ•°æ®é‡ä¼˜åŒ–ç‰ˆ)
        
        Args:
            df: åŸå§‹DataFrame
            n_jobs: å¹¶è¡Œä»»åŠ¡æ•°ï¼Œ-1è¡¨ç¤ºä½¿ç”¨æ‰€æœ‰CPUæ ¸å¿ƒ
        """
        print("   å¹¶è¡Œé¢„å¤„ç†ä¸­...")
        
        # ç§»é™¤ç©ºè¯„è®º
        df = df.dropna(subset=['text'])
        df = df[df['text'].str.len() > 0]
        
        # é•¿åº¦è¿‡æ»¤
        min_len = Config.PREPROCESSING.get('min_comment_length', 10)
        max_len = Config.PREPROCESSING.get('max_comment_length', 5000)
        df = df[df['text'].str.len().between(min_len, max_len)]
        
        # ç§»é™¤[deleted]å’Œ[removed]
        df = df[~df['text'].str.contains(r'^\[deleted\]$|^\[removed\]$', regex=True, na=False)]
        
        total = len(df)
        print(f"   è¿‡æ»¤å: {total:,} æ¡å¾…å¤„ç†")
        
        # å¯¹äºå¤§æ•°æ®é›†ï¼Œä½¿ç”¨æ‰¹é‡å¤„ç†è€Œéå®Œå…¨å¹¶è¡Œï¼ˆé¿å…å†…å­˜é—®é¢˜ï¼‰
        batch_size = 50000
        all_clean_texts = []
        all_tokens = []
        
        for batch_start in range(0, total, batch_size):
            batch_end = min(batch_start + batch_size, total)
            batch_df = df.iloc[batch_start:batch_end]
            
            # æ‰¹é‡å¤„ç†æ–‡æœ¬
            batch_texts = batch_df['text'].tolist()
            batch_clean = [self.preprocessor.clean_text(t) for t in batch_texts]
            batch_tokens = [self.preprocessor.tokenize(t) for t in batch_clean]
            
            all_clean_texts.extend(batch_clean)
            all_tokens.extend(batch_tokens)
            
            print(f"      è¿›åº¦: {batch_end:,}/{total:,} ({100*batch_end/total:.1f}%)")
        
        df = df.copy()
        df['clean_text'] = all_clean_texts
        df['tokens'] = all_tokens
        df['token_count'] = df['tokens'].apply(len)
        
        # ç§»é™¤tokenè¿‡å°‘çš„è¯„è®º
        df = df[df['token_count'] >= 3]
        
        # é‡ç½®ç´¢å¼•
        df = df.reset_index(drop=True)
        
        return df
        
        # ç§»é™¤ç©ºè¯„è®º
        df = df.dropna(subset=['text'])
        df = df[df['text'].str.len() > 0]
        
        # é•¿åº¦è¿‡æ»¤
        min_len = Config.PREPROCESSING.get('min_comment_length', 10)
        max_len = Config.PREPROCESSING.get('max_comment_length', 5000)
        df = df[df['text'].str.len().between(min_len, max_len)]
        
        # ç§»é™¤[deleted]å’Œ[removed]
        df = df[~df['text'].str.contains(r'^\[deleted\]$|^\[removed\]$', regex=True, na=False)]
        
        # æ·»åŠ å¤„ç†åçš„æ–‡æœ¬åˆ—
        df['clean_text'] = df['text'].apply(self.preprocessor.clean_text)
        df['tokens'] = df['clean_text'].apply(lambda x: self.preprocessor.tokenize(x))
        df['token_count'] = df['tokens'].apply(len)
        
        # ç§»é™¤tokenè¿‡å°‘çš„è¯„è®º
        df = df[df['token_count'] >= 3]
        
        # é‡ç½®ç´¢å¼•
        df = df.reset_index(drop=True)
        
        return df
    
    def _generate_sample_data(self, num_comments: int) -> pd.DataFrame:
        """ç”Ÿæˆæ¨¡æ‹Ÿæ•°æ®(å½“çœŸå®æ•°æ®ä¸å¯ç”¨æ—¶)"""
        print(f"   ç”Ÿæˆ {num_comments:,} æ¡æ¨¡æ‹Ÿè¯„è®º...")
        
        # æ¨¡æ‹Ÿçš„subreddits
        subreddits = [
            'technology', 'politics', 'gaming', 'movies', 'news',
            'worldnews', 'science', 'AskReddit', 'funny', 'pics'
        ]
        
        # è¯é¢˜æ¨¡æ¿
        topics = {
            'technology': [
                "AI is going to change everything in the next few years",
                "This new smartphone is actually pretty impressive",
                "I don't trust tech companies with my data anymore",
                "The future of computing is quantum",
                "Self-driving cars are still not ready for mainstream",
                "Open source software is the way to go",
                "This is just another tech bubble waiting to burst",
            ],
            'politics': [
                "The government needs to do more about this issue",
                "I can't believe they passed that bill",
                "Both sides are missing the point here",
                "This is exactly what I voted for",
                "Politicians never keep their promises",
                "We need more transparency in government",
            ],
            'gaming': [
                "This game is absolutely amazing, best I've played",
                "The graphics are insane but gameplay is meh",
                "Can't believe they're charging full price for this",
                "The community for this game is so toxic",
                "I've been playing for 500 hours, no regrets",
                "This is just a reskin of the previous game",
            ],
            'movies': [
                "Best movie I've seen this year, hands down",
                "The plot twist was so predictable",
                "Great acting but terrible writing",
                "This deserves all the awards",
                "I don't understand the hype around this movie",
                "The original was way better",
            ],
            'news': [
                "This is a developing story, stay tuned",
                "Can we get a reliable source for this?",
                "Not surprised, saw this coming",
                "This affects more people than you'd think",
                "Media is blowing this out of proportion",
            ],
        }
        
        # é€šç”¨å›å¤æ¨¡æ¿
        generic_replies = [
            "I completely agree with this",
            "This is so wrong on many levels",
            "Can confirm, I've experienced the same thing",
            "Source?",
            "This deserves more upvotes",
            "Underrated comment right here",
            "First time I've heard about this",
            "Thanks for sharing your perspective",
            "I used to think this way but changed my mind",
            "This is exactly what I was thinking",
            "People really need to understand this better",
            "I disagree but I see where you're coming from",
            "This needs to be higher up",
            "Same here, it's frustrating",
            "Great point, never thought about it that way",
        ]
        
        data = []
        np.random.seed(42)
        
        for i in range(num_comments):
            subreddit = np.random.choice(subreddits)
            
            # 70% æ¥è‡ªè¯é¢˜æ¨¡æ¿, 30% é€šç”¨å›å¤
            if np.random.random() < 0.7 and subreddit in topics:
                base_text = np.random.choice(topics.get(subreddit, topics['technology']))
            else:
                base_text = np.random.choice(generic_replies)
            
            # æ·»åŠ ä¸€äº›å˜åŒ–
            variations = [
                "",
                " honestly",
                " tbh",
                " imo",
                " lol",
                "!",
                "...",
                " definitely",
            ]
            text = base_text + np.random.choice(variations)
            
            # 15% é‡å¤è¯„è®º(æ¨¡æ‹Ÿå¤è¯»æœº)
            if np.random.random() < 0.15 and len(data) > 0:
                text = np.random.choice(data)['text']
                # å¯èƒ½æœ‰è½»å¾®ä¿®æ”¹
                if np.random.random() < 0.5:
                    text = text.replace('.', '!').replace(',', '')
            
            data.append({
                'id': i,
                'text': text,
                'subreddit': subreddit,
                'score': int(np.random.exponential(10)),
                'author': f'user_{np.random.randint(1, 10000)}',
                'timestamp': pd.Timestamp.now() - pd.Timedelta(hours=np.random.randint(0, 168)),
            })
        
        df = pd.DataFrame(data)
        df = self._preprocess(df)
        
        self.subreddits = subreddits
        self.df = df
        
        return df
    
    def get_comments_by_subreddit(self, subreddit: str) -> pd.DataFrame:
        """è·å–ç‰¹å®šsubredditçš„è¯„è®º"""
        if self.df is None:
            raise ValueError("è¯·å…ˆè°ƒç”¨load()åŠ è½½æ•°æ®")
        return self.df[self.df['subreddit'] == subreddit]
    
    def get_subreddit_stats(self) -> pd.DataFrame:
        """è·å–å„subredditçš„ç»Ÿè®¡ä¿¡æ¯"""
        if self.df is None:
            raise ValueError("è¯·å…ˆè°ƒç”¨load()åŠ è½½æ•°æ®")
        
        stats = self.df.groupby('subreddit').agg({
            'id': 'count',
            'score': ['mean', 'sum'],
            'token_count': 'mean',
        }).round(2)
        
        stats.columns = ['count', 'avg_score', 'total_score', 'avg_tokens']
        stats = stats.sort_values('count', ascending=False)
        
        return stats
    
    def iterate_batches(self, batch_size: int = 10000) -> Generator[pd.DataFrame, None, None]:
        """æ‰¹é‡è¿­ä»£æ•°æ®(ç”¨äºå¤§è§„æ¨¡å¤„ç†)"""
        if self.df is None:
            raise ValueError("è¯·å…ˆè°ƒç”¨load()åŠ è½½æ•°æ®")
        
        for i in range(0, len(self.df), batch_size):
            yield self.df.iloc[i:i+batch_size]
    
    def to_documents(self) -> List[Dict]:
        """è½¬æ¢ä¸ºæ–‡æ¡£åˆ—è¡¨æ ¼å¼"""
        if self.df is None:
            raise ValueError("è¯·å…ˆè°ƒç”¨load()åŠ è½½æ•°æ®")
        
        return self.df.to_dict('records')
