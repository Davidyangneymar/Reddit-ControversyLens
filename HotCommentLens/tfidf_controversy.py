"""
TF-IDFå…³é”®è¯æå– & äº‰è®®æ€§åˆ†ææ¨¡å—

==========================================================================
å¤§æ•°æ®åˆ†ææ–¹æ³•è¯´æ˜:
==========================================================================

1. TF-IDF (Term Frequency - Inverse Document Frequency)
   -------------------------------------------------------
   - TF (è¯é¢‘): è¯åœ¨æ–‡æ¡£ä¸­å‡ºç°çš„æ¬¡æ•° / æ–‡æ¡£æ€»è¯æ•°
   - IDF (é€†æ–‡æ¡£é¢‘ç‡): log(æ€»æ–‡æ¡£æ•° / åŒ…å«è¯¥è¯çš„æ–‡æ¡£æ•°)
   - TF-IDF = TF Ã— IDF
   - ä½œç”¨: è¯†åˆ«åœ¨ç‰¹å®šç¤¾åŒºä¸­é‡è¦ä½†å…¨å±€ä¸å¸¸è§çš„å…³é”®è¯
   - ä¼˜ç‚¹: è‡ªåŠ¨é™ä½å¸¸è§è¯æƒé‡ï¼Œçªå‡ºç‰¹è‰²è¯æ±‡

2. MapReduce æ¨¡å¼
   -------------------------------------------------------
   - Mapé˜¶æ®µ: å¯¹æ¯æ¡è¯„è®ºè¿›è¡Œåˆ†è¯ï¼Œæå–(word, 1)é”®å€¼å¯¹
   - Reduceé˜¶æ®µ: èšåˆç›¸åŒè¯çš„è®¡æ•°ï¼Œå¾—åˆ°è¯é¢‘ç»Ÿè®¡
   - åº”ç”¨åœºæ™¯: å¤§è§„æ¨¡æ–‡æœ¬è¯é¢‘ç»Ÿè®¡ã€åˆ†ç»„èšåˆ
   - æœ¬é¡¹ç›®åº”ç”¨: æŒ‰subredditåˆ†ç»„ç»Ÿè®¡è¯é¢‘

3. æ‰¹é‡å¤„ç† (Batch Processing)
   -------------------------------------------------------
   - å°†100ä¸‡æ¡æ•°æ®åˆ†æˆå¤šä¸ªå°æ‰¹æ¬¡å¤„ç†
   - æ¯æ‰¹å¤„ç†å®Œåé‡Šæ”¾å†…å­˜
   - é¿å…ä¸€æ¬¡æ€§åŠ è½½å¯¼è‡´å†…å­˜æº¢å‡º

4. å‘é‡åŒ–æ“ä½œ (Vectorization)
   -------------------------------------------------------
   - ä½¿ç”¨pandas/numpyæ›¿ä»£Pythonå¾ªç¯
   - åˆ©ç”¨åº•å±‚Cä¼˜åŒ–ï¼Œæå‡10-100å€æ€§èƒ½
   - ä¾‹: df['length'] = df['text'].str.len()

5. åˆ†å±‚æŠ½æ · (Stratified Sampling)
   -------------------------------------------------------
   - ä»äº‰è®®æ€§/éäº‰è®®æ€§è¯„è®ºä¸­æŒ‰æ¯”ä¾‹æŠ½æ ·
   - ä¿è¯å¯¹æ¯”åˆ†æçš„å…¬å¹³æ€§å’Œä»£è¡¨æ€§
==========================================================================
"""

import os
import re
import time
import pandas as pd
import numpy as np
from typing import List, Dict, Tuple, Optional
from collections import Counter
from dataclasses import dataclass, field
import matplotlib.pyplot as plt

# é…ç½®
import sys
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

try:
    from .config import Config
except ImportError:
    from config import Config


@dataclass
class ControversyStats:
    """äº‰è®®æ€§ç»Ÿè®¡ç»“æœ"""
    total_comments: int = 0
    controversial_count: int = 0
    non_controversial_count: int = 0
    controversy_rate: float = 0.0
    
    # ç‰¹å¾å¯¹æ¯”
    avg_length_controversial: float = 0.0
    avg_length_non_controversial: float = 0.0
    avg_score_controversial: float = 0.0
    avg_score_non_controversial: float = 0.0
    
    # å…³é”®è¯
    controversial_keywords: List[Tuple[str, float]] = field(default_factory=list)
    non_controversial_keywords: List[Tuple[str, float]] = field(default_factory=list)
    
    # æŒ‰subredditç»Ÿè®¡
    subreddit_controversy_rates: Dict[str, float] = field(default_factory=dict)


class TFIDFAnalyzer:
    """
    TF-IDFå…³é”®è¯æå–å™¨
    
    å¤§æ•°æ®æ–¹æ³•:
    ===========
    1. MapReduceæ¨¡å¼è¿›è¡Œè¯é¢‘ç»Ÿè®¡
    2. æ‰¹é‡å¤„ç†é¿å…å†…å­˜æº¢å‡º
    3. å‘é‡åŒ–æ“ä½œåŠ é€Ÿè®¡ç®—
    """
    
    def __init__(self):
        self.stopwords = self._get_stopwords()
    
    def _get_stopwords(self) -> set:
        """è·å–åœç”¨è¯è¡¨"""
        base_stopwords = {
            'i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you',
            "you're", "you've", "you'll", "you'd", 'your', 'yours', 'yourself',
            'yourselves', 'he', 'him', 'his', 'himself', 'she', "she's", 'her',
            'hers', 'herself', 'it', "it's", 'its', 'itself', 'they', 'them',
            'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom',
            'this', 'that', "that'll", 'these', 'those', 'am', 'is', 'are',
            'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having',
            'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if',
            'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for',
            'with', 'about', 'against', 'between', 'into', 'through', 'during',
            'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down',
            'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further',
            'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how',
            'all', 'each', 'few', 'more', 'most', 'other', 'some', 'such',
            'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too',
            'very', 's', 't', 'can', 'will', 'just', 'don', "don't", 'should',
            "should've", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y',
        }
        
        extra_stopwords = {
            'would', 'could', 'get', 'got', 'like', 'know', 'think',
            'really', 'even', 'well', 'also', 'still', 'way', 'much',
            'thing', 'things', 'something', 'anything', 'nothing',
            'people', 'person', 'one', 'two', 'first', 'new', 'good',
            'make', 'made', 'see', 'want', 'say', 'said', 'going',
            'take', 'come', 'came', 'look', 'use', 'used', 'time',
            'yeah', 'yes', 'okay', 'actually', 'probably', 'maybe',
            'right', 'need', 'mean', 'sure', 'lot', 'back',
            'thats', 'dont', 'doesnt', 'didnt', 'cant', 'wont', 'isnt',
            'im', 'ive', 'youre', 'hes', 'shes', 'theyre', 'wasnt',
            'deleted', 'removed', 'comment', 'edit', 'reddit', 'sub',
            'http', 'https', 'www', 'com', 'org', 'amp', 'any',
        }
        
        return base_stopwords | extra_stopwords
    
    def compute_tfidf(
        self, 
        df: pd.DataFrame,
        text_column: str = 'body',
        group_column: str = 'subreddit',
        top_n: int = 20
    ) -> Dict[str, List[Tuple[str, float]]]:
        """
        è®¡ç®—å„ç»„çš„TF-IDFå…³é”®è¯
        
        å¤§æ•°æ®æ–¹æ³•:
        -----------
        1. MapReduce: 
           - Map: æ¯ä¸ªæ–‡æ¡£åˆ†è¯ -> (word, doc_id)
           - Reduce: æŒ‰ç»„èšåˆè¯é¢‘
        
        2. å‘é‡åŒ–: ä½¿ç”¨pandas groupbyè¿›è¡Œé«˜æ•ˆåˆ†ç»„
        
        Args:
            df: æ•°æ®DataFrame
            text_column: æ–‡æœ¬åˆ—å
            group_column: åˆ†ç»„åˆ—å
            top_n: æ¯ç»„è¿”å›çš„å…³é”®è¯æ•°é‡
            
        Returns:
            {group: [(word, tfidf_score), ...]}
        """
        print("\n" + "="*60)
        print("ğŸ“Š TF-IDF å…³é”®è¯æå–")
        print("="*60)
        print("\nğŸ”§ å¤§æ•°æ®æ–¹æ³•: MapReduce + å‘é‡åŒ–æ“ä½œ")
        
        groups = df[group_column].unique()
        total_docs = len(df)
        print(f"\nğŸ“ åˆ†ç»„æ•°: {len(groups)}")
        print(f"ğŸ“„ æ€»æ–‡æ¡£æ•°: {total_docs:,}")
        
        # ============ Mapé˜¶æ®µ: ç»Ÿè®¡æ¯ç»„è¯é¢‘ ============
        print("\nğŸ—ºï¸  [Mapé˜¶æ®µ] ç»Ÿè®¡å„ç»„è¯é¢‘...")
        group_word_counts = {}  # {group: Counter}
        group_doc_counts = {}   # {group: doc_count}
        
        for group in groups:
            group_df = df[df[group_column] == group]
            group_doc_counts[group] = len(group_df)
            
            # æ‰¹é‡ç»Ÿè®¡è¯é¢‘
            word_counter = Counter()
            for text in group_df[text_column]:
                if isinstance(text, str):
                    words = re.findall(r'\b[a-z]{3,15}\b', text.lower())
                    words = [w for w in words if w not in self.stopwords]
                    # ä½¿ç”¨setå»é‡ï¼Œç»Ÿè®¡æ–‡æ¡£é¢‘ç‡(DF)
                    word_counter.update(set(words))
            
            group_word_counts[group] = word_counter
        
        # ============ è®¡ç®—å…¨å±€æ–‡æ¡£é¢‘ç‡(IDF) ============
        print("ğŸ“‰ [è®¡ç®—IDF] ç»Ÿè®¡å…¨å±€æ–‡æ¡£é¢‘ç‡...")
        global_doc_freq = Counter()
        for counter in group_word_counts.values():
            global_doc_freq.update(counter.keys())
        
        # ============ Reduceé˜¶æ®µ: è®¡ç®—TF-IDF ============
        print("ğŸ”¢ [Reduceé˜¶æ®µ] è®¡ç®—TF-IDFå¾—åˆ†...")
        tfidf_results = {}
        
        for group in groups:
            word_counts = group_word_counts[group]
            group_total = sum(word_counts.values())
            
            if group_total == 0:
                tfidf_results[group] = []
                continue
            
            word_scores = []
            for word, count in word_counts.items():
                # TF: è¯é¢‘ / æ€»è¯æ•°
                tf = count / group_total
                
                # IDF: log(æ€»æ–‡æ¡£æ•° / åŒ…å«è¯¥è¯çš„æ–‡æ¡£æ•°)
                df_count = global_doc_freq.get(word, 1)
                idf = np.log(total_docs / df_count)
                
                # TF-IDF
                tfidf = tf * idf
                
                if count >= 5:  # æœ€å°é¢‘ç‡é˜ˆå€¼
                    word_scores.append((word, tfidf, count))
            
            # æ’åºå–top_n
            word_scores.sort(key=lambda x: x[1], reverse=True)
            tfidf_results[group] = [(w, score) for w, score, _ in word_scores[:top_n]]
        
        print("âœ… TF-IDFè®¡ç®—å®Œæˆ!")
        return tfidf_results


class ControversyAnalyzer:
    """
    äº‰è®®æ€§åˆ†æå™¨
    
    å¤§æ•°æ®æ–¹æ³•:
    ===========
    1. å‘é‡åŒ–æ“ä½œ: pandaså¸ƒå°”ç´¢å¼•ã€groupbyèšåˆ
    2. MapReduce: åˆ†ç»„ç»Ÿè®¡å„æŒ‡æ ‡
    3. åˆ†å±‚æŠ½æ ·: å¯¹æ¯”åˆ†ææ—¶ä¿è¯æ ·æœ¬å¹³è¡¡
    """
    
    def __init__(self):
        self.stopwords = TFIDFAnalyzer()._get_stopwords()
    
    def analyze(self, df: pd.DataFrame, text_column: str = 'body') -> ControversyStats:
        """
        æ‰§è¡Œäº‰è®®æ€§åˆ†æ
        
        å¤§æ•°æ®æ–¹æ³•:
        -----------
        - å‘é‡åŒ–: ä½¿ç”¨pandaså‘é‡æ“ä½œæ›¿ä»£å¾ªç¯
        - MapReduce: groupby = Map, agg = Reduce
        """
        print("\n" + "="*60)
        print("ğŸ”¥ äº‰è®®æ€§åˆ†æ (Controversy Analysis)")
        print("="*60)
        print("\nğŸ”§ å¤§æ•°æ®æ–¹æ³•: å‘é‡åŒ–æ“ä½œ + MapReduceèšåˆ")
        
        stats = ControversyStats()
        
        # ============ 1. åŸºç¡€ç»Ÿè®¡ (å‘é‡åŒ–) ============
        print("\nğŸ“Š [1/4] åŸºç¡€ç»Ÿè®¡ (å‘é‡åŒ–æ“ä½œ)...")
        stats.total_comments = len(df)
        
        # å¸ƒå°”ç´¢å¼• - å‘é‡åŒ–æ“ä½œ
        controversial_mask = df['controversiality'] == 1
        stats.controversial_count = controversial_mask.sum()
        stats.non_controversial_count = stats.total_comments - stats.controversial_count
        stats.controversy_rate = stats.controversial_count / stats.total_comments * 100
        
        print(f"   æ€»è¯„è®ºæ•°: {stats.total_comments:,}")
        print(f"   äº‰è®®æ€§è¯„è®º: {stats.controversial_count:,} ({stats.controversy_rate:.2f}%)")
        print(f"   éäº‰è®®æ€§è¯„è®º: {stats.non_controversial_count:,}")
        
        # ============ 2. ç‰¹å¾å¯¹æ¯” (å‘é‡åŒ–) ============
        print("\nğŸ“ [2/4] ç‰¹å¾å¯¹æ¯” (å‘é‡åŒ–è®¡ç®—)...")
        
        # å‘é‡åŒ–è®¡ç®—è¯„è®ºé•¿åº¦
        df = df.copy()
        df['comment_length'] = df[text_column].str.len()
        
        controversial_df = df[controversial_mask]
        non_controversial_df = df[~controversial_mask]
        
        stats.avg_length_controversial = controversial_df['comment_length'].mean()
        stats.avg_length_non_controversial = non_controversial_df['comment_length'].mean()
        stats.avg_score_controversial = controversial_df['score'].mean()
        stats.avg_score_non_controversial = non_controversial_df['score'].mean()
        
        print(f"   äº‰è®®è¯„è®ºå¹³å‡é•¿åº¦: {stats.avg_length_controversial:.1f} å­—ç¬¦")
        print(f"   éäº‰è®®è¯„è®ºå¹³å‡é•¿åº¦: {stats.avg_length_non_controversial:.1f} å­—ç¬¦")
        print(f"   äº‰è®®è¯„è®ºå¹³å‡å¾—åˆ†: {stats.avg_score_controversial:.2f}")
        print(f"   éäº‰è®®è¯„è®ºå¹³å‡å¾—åˆ†: {stats.avg_score_non_controversial:.2f}")
        
        # ============ 3. æŒ‰Subredditç»Ÿè®¡ (MapReduce) ============
        print("\nğŸ“ [3/4] æŒ‰Subredditç»Ÿè®¡ (MapReduce: groupby+agg)...")
        
        # groupby = Map, agg = Reduce
        subreddit_stats = df.groupby('subreddit').agg({
            'controversiality': ['sum', 'count']
        })
        subreddit_stats.columns = ['controversial', 'total']
        subreddit_stats['rate'] = subreddit_stats['controversial'] / subreddit_stats['total'] * 100
        subreddit_stats = subreddit_stats.sort_values('rate', ascending=False)
        
        stats.subreddit_controversy_rates = subreddit_stats['rate'].to_dict()
        
        print(f"   æœ€å…·äº‰è®®æ€§çš„Subreddit Top 5:")
        for i, (sub, row) in enumerate(subreddit_stats.head(5).iterrows()):
            print(f"   {i+1}. r/{sub}: {row['rate']:.2f}% ({int(row['controversial'])}/{int(row['total'])})")
        
        # ============ 4. æå–å·®å¼‚å…³é”®è¯ (åˆ†å±‚æŠ½æ · + MapReduce) ============
        print("\nğŸ”‘ [4/4] å·®å¼‚å…³é”®è¯ (åˆ†å±‚æŠ½æ · + MapReduce)...")
        stats.controversial_keywords, stats.non_controversial_keywords = \
            self._extract_differential_keywords(
                controversial_df, non_controversial_df, text_column
            )
        
        print(f"   äº‰è®®æ€§é«˜é¢‘è¯: {', '.join([w for w, _ in stats.controversial_keywords[:8]])}")
        print(f"   éäº‰è®®æ€§é«˜é¢‘è¯: {', '.join([w for w, _ in stats.non_controversial_keywords[:8]])}")
        
        return stats
    
    def _extract_differential_keywords(
        self,
        controversial_df: pd.DataFrame,
        non_controversial_df: pd.DataFrame,
        text_column: str,
        sample_size: int = 50000
    ) -> Tuple[List[Tuple[str, float]], List[Tuple[str, float]]]:
        """
        æå–å·®å¼‚æ€§å…³é”®è¯
        
        å¤§æ•°æ®æ–¹æ³•:
        -----------
        - åˆ†å±‚æŠ½æ ·: å¹³è¡¡ä¸¤ç±»æ ·æœ¬
        - MapReduce: æ‰¹é‡è¯é¢‘ç»Ÿè®¡
        """
        # åˆ†å±‚æŠ½æ ·
        if len(controversial_df) > sample_size:
            controversial_sample = controversial_df.sample(n=sample_size, random_state=42)
        else:
            controversial_sample = controversial_df
            
        if len(non_controversial_df) > sample_size:
            non_controversial_sample = non_controversial_df.sample(n=sample_size, random_state=42)
        else:
            non_controversial_sample = non_controversial_df
        
        # MapReduceè¯é¢‘ç»Ÿè®¡
        controversial_words = self._count_words(controversial_sample[text_column])
        non_controversial_words = self._count_words(non_controversial_sample[text_column])
        
        # è®¡ç®—å·®å¼‚æ€§å¾—åˆ†
        controversial_kw = self._compute_differential_score(controversial_words, non_controversial_words)
        non_controversial_kw = self._compute_differential_score(non_controversial_words, controversial_words)
        
        return controversial_kw, non_controversial_kw
    
    def _count_words(self, texts: pd.Series) -> Counter:
        """æ‰¹é‡ç»Ÿè®¡è¯é¢‘ (MapReduce)"""
        counter = Counter()
        for text in texts:
            if isinstance(text, str):
                words = re.findall(r'\b[a-z]{3,15}\b', text.lower())
                words = [w for w in words if w not in self.stopwords]
                counter.update(words)
        return counter
    
    def _compute_differential_score(
        self,
        target_counter: Counter,
        background_counter: Counter,
        top_n: int = 50
    ) -> List[Tuple[str, float]]:
        """è®¡ç®—å·®å¼‚æ€§å¾—åˆ†"""
        target_total = sum(target_counter.values())
        background_total = sum(background_counter.values())
        
        if target_total == 0 or background_total == 0:
            return []
        
        scores = []
        for word, count in target_counter.most_common(300):
            target_freq = count / target_total
            background_freq = (background_counter.get(word, 0) + 1) / (background_total + 1)
            diff_score = target_freq / background_freq
            
            if count >= 10:
                scores.append((word, diff_score))
        
        scores.sort(key=lambda x: x[1], reverse=True)
        return scores[:top_n]


class AnalysisVisualizer:
    """åˆ†æç»“æœå¯è§†åŒ–"""
    
    def __init__(self, output_dir: str):
        self.output_dir = output_dir
        self.viz_dir = os.path.join(output_dir, "visualizations")
        os.makedirs(self.viz_dir, exist_ok=True)
        
        plt.rcParams['font.sans-serif'] = ['SimHei', 'DejaVu Sans']
        plt.rcParams['axes.unicode_minus'] = False
    
    def plot_tfidf_keywords(
        self,
        tfidf_results: Dict[str, List[Tuple[str, float]]],
        top_n_groups: int = 6,
        top_n_keywords: int = 10
    ) -> str:
        """ç»˜åˆ¶TF-IDFå…³é”®è¯å›¾"""
        # é€‰å–å…³é”®è¯æœ€å¤šçš„ç»„
        groups = sorted(tfidf_results.keys(),
                       key=lambda x: len(tfidf_results[x]),
                       reverse=True)[:top_n_groups]
        
        n_cols = 2
        n_rows = (len(groups) + n_cols - 1) // n_cols
        
        fig, axes = plt.subplots(n_rows, n_cols, figsize=(14, 4*n_rows))
        axes = axes.flatten()
        colors = plt.cm.tab10(np.linspace(0, 1, len(groups)))
        
        for idx, group in enumerate(groups):
            ax = axes[idx]
            keywords = tfidf_results[group][:top_n_keywords]
            
            if not keywords:
                ax.text(0.5, 0.5, 'No keywords', ha='center', va='center')
                ax.set_title(f'r/{group}')
                continue
            
            words = [w for w, _ in keywords]
            scores = [s for _, s in keywords]
            
            y_pos = np.arange(len(words))
            ax.barh(y_pos, scores, color=colors[idx], alpha=0.8)
            ax.set_yticks(y_pos)
            ax.set_yticklabels(words)
            ax.set_xlabel('TF-IDF Score')
            ax.set_title(f'r/{group}', fontsize=11, fontweight='bold')
            ax.invert_yaxis()
        
        for idx in range(len(groups), len(axes)):
            axes[idx].axis('off')
        
        plt.suptitle('TF-IDF Keywords by Subreddit', fontsize=14, fontweight='bold', y=1.02)
        plt.tight_layout()
        
        filepath = os.path.join(self.viz_dir, 'tfidf_keywords.png')
        plt.savefig(filepath, dpi=150, bbox_inches='tight')
        plt.close()
        print(f"   ä¿å­˜: {filepath}")
        return filepath
    
    def plot_controversy_stats(self, stats: ControversyStats) -> str:
        """ç»˜åˆ¶äº‰è®®æ€§ç»Ÿè®¡å›¾"""
        fig, axes = plt.subplots(2, 2, figsize=(14, 10))
        
        # 1. é¥¼å›¾
        ax1 = axes[0, 0]
        sizes = [stats.controversial_count, stats.non_controversial_count]
        labels = [f'Controversial\n({stats.controversial_count:,})',
                  f'Non-controversial\n({stats.non_controversial_count:,})']
        colors = ['#e74c3c', '#3498db']
        ax1.pie(sizes, labels=labels, colors=colors, autopct='%1.1f%%',
                startangle=90, explode=(0.05, 0))
        ax1.set_title('Comment Distribution', fontsize=12, fontweight='bold')
        
        # 2. é•¿åº¦å¯¹æ¯”
        ax2 = axes[0, 1]
        categories = ['Controversial', 'Non-controversial']
        lengths = [stats.avg_length_controversial, stats.avg_length_non_controversial]
        bars = ax2.bar(categories, lengths, color=['#e74c3c', '#3498db'])
        ax2.set_ylabel('Average Length (chars)')
        ax2.set_title('Comment Length Comparison', fontsize=12, fontweight='bold')
        for bar, val in zip(bars, lengths):
            ax2.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 3,
                    f'{val:.0f}', ha='center', fontsize=10)
        
        # 3. å¾—åˆ†å¯¹æ¯”
        ax3 = axes[1, 0]
        scores = [stats.avg_score_controversial, stats.avg_score_non_controversial]
        bars = ax3.bar(categories, scores, color=['#e74c3c', '#3498db'])
        ax3.set_ylabel('Average Score')
        ax3.set_title('Comment Score Comparison', fontsize=12, fontweight='bold')
        ax3.axhline(y=0, color='black', linestyle='-', linewidth=0.5)
        for bar, val in zip(bars, scores):
            ax3.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.2,
                    f'{val:.2f}', ha='center', fontsize=10)
        
        # 4. Topäº‰è®®subreddit
        ax4 = axes[1, 1]
        top_subs = dict(sorted(stats.subreddit_controversy_rates.items(),
                               key=lambda x: x[1], reverse=True)[:10])
        subs = list(top_subs.keys())
        rates = list(top_subs.values())
        
        y_pos = np.arange(len(subs))
        ax4.barh(y_pos, rates, color='#e74c3c', alpha=0.8)
        ax4.set_yticks(y_pos)
        ax4.set_yticklabels([f'r/{s}' for s in subs])
        ax4.set_xlabel('Controversy Rate (%)')
        ax4.set_title('Top 10 Controversial Subreddits', fontsize=12, fontweight='bold')
        ax4.invert_yaxis()
        
        for bar, val in zip(ax4.patches, rates):
            ax4.text(bar.get_width() + 0.1, bar.get_y() + bar.get_height()/2,
                    f'{val:.1f}%', va='center', fontsize=9)
        
        plt.tight_layout()
        
        filepath = os.path.join(self.viz_dir, 'controversy_analysis.png')
        plt.savefig(filepath, dpi=150, bbox_inches='tight')
        plt.close()
        print(f"   ä¿å­˜: {filepath}")
        return filepath
    
    def plot_keyword_comparison(
        self,
        controversial_kw: List[Tuple[str, float]],
        non_controversial_kw: List[Tuple[str, float]],
        top_n: int = 15
    ) -> str:
        """ç»˜åˆ¶å…³é”®è¯å¯¹æ¯”å›¾"""
        fig, axes = plt.subplots(1, 2, figsize=(14, 8))
        
        # äº‰è®®æ€§å…³é”®è¯
        ax1 = axes[0]
        words1 = [w for w, _ in controversial_kw[:top_n]]
        scores1 = [s for _, s in controversial_kw[:top_n]]
        y_pos = np.arange(len(words1))
        ax1.barh(y_pos, scores1, color='#e74c3c', alpha=0.8)
        ax1.set_yticks(y_pos)
        ax1.set_yticklabels(words1)
        ax1.set_xlabel('Differential Score')
        ax1.set_title('Controversial Keywords', fontsize=12, fontweight='bold')
        ax1.invert_yaxis()
        
        # éäº‰è®®æ€§å…³é”®è¯
        ax2 = axes[1]
        words2 = [w for w, _ in non_controversial_kw[:top_n]]
        scores2 = [s for _, s in non_controversial_kw[:top_n]]
        y_pos = np.arange(len(words2))
        ax2.barh(y_pos, scores2, color='#3498db', alpha=0.8)
        ax2.set_yticks(y_pos)
        ax2.set_yticklabels(words2)
        ax2.set_xlabel('Differential Score')
        ax2.set_title('Non-controversial Keywords', fontsize=12, fontweight='bold')
        ax2.invert_yaxis()
        
        plt.tight_layout()
        
        filepath = os.path.join(self.viz_dir, 'keyword_comparison.png')
        plt.savefig(filepath, dpi=150, bbox_inches='tight')
        plt.close()
        print(f"   ä¿å­˜: {filepath}")
        return filepath
